[package]
name = "llmur-proxy"
version = "0.0.1"
edition = "2024"
authors = ["andremachado94"]
license = "MIT"
description = "Lightweight LLM Proxy - Binary application for running an LLM proxy server"
repository = "https://github.com/llmur/llmur"
homepage = "https://github.com/llmur/llmur"
keywords = ["llm", "proxy", "openai", "api", "ai"]
categories = ["command-line-utilities"]
readme = "../README.md"

[[bin]]
name = "llmur-proxy"
path = "src/main.rs"

[dependencies]
llmur = { path = "../llmur" }

tokio = { workspace = true, features = ["rt", "rt-multi-thread", "macros"] }
serde = { workspace = true }
serde_yaml = { workspace = true }
uuid = { workspace = true }
axum = { workspace = true }

async-trait = { workspace = true }


tracing = { workspace = true }
tracing-subscriber = { workspace = true }
opentelemetry = { workspace = true }
opentelemetry_sdk = { workspace = true }
opentelemetry-otlp = { workspace = true }
opentelemetry-semantic-conventions = { workspace = true }
opentelemetry-appender-tracing = { workspace = true }

tracing-opentelemetry = { workspace = true }

clap = "4.5.18"
clap_derive = "4.5.18"
tower-http = { workspace = true }
