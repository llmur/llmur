use serde::{Deserialize, Serialize};

// region: --- Response structs
/// Azure OpenAI chat completion response payload.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct Response {
    pub choices: Vec<ResponseChoice>,
    pub created: u64,
    pub id: String,
    pub model: String,
    pub object: String,
    pub system_fingerprint: Option<String>,
    pub usage: ResponseUsage,
}

/// Token usage metadata for a chat completion.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseUsage {
    pub completion_tokens: u64,
    pub prompt_tokens: u64,
    pub total_tokens: u64,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub completion_tokens_details: Option<CompletionTokensDetails>
}

/// Completion token details (reasoning tokens when available).
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct CompletionTokensDetails {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub reasoning_tokens: Option<u64>,
}

/// Response choice with message, finish reason, and filter results.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseChoice {
    pub finish_reason: String,
    pub index: u64,
    pub message: ResponseChoiceMessage,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content_filter_results: Option<ResponseContentFilterResults>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub logprobs: Option<ResponseChoiceLogprob>,
}

/// Content filter results for a response choice.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseContentFilterResults {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub sexual: Option<ContentFilterSeverityResult>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub violence: Option<ContentFilterSeverityResult>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub hate: Option<ContentFilterSeverityResult>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub self_harm: Option<ContentFilterSeverityResult>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub profanity: Option<ContentFilterDetectedResult>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub error: Option<ErrorBase>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub protected_material_text: Option<ContentFilterDetectedResult>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub protected_material_code: Option<ContentFilterDetectedWithCitationResult>,
}

/// Response message generated by the model.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseChoiceMessage {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<String>,
    pub role: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub refusal: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_calls: Option<Vec<ResponseChoiceToolCall>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub context: Option<AzureMessageContext>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub function_call: Option<ResponseFunctionCall>,
}

/// Azure extensions context attached to a response message.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct AzureMessageContext {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub intent: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub citations: Option<Vec<AzureContextCitation>>,
}

/// Citation entry in Azure extensions context.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct AzureContextCitation {
    pub content: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub title: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub url: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub filepath: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub chunk_id: Option<String>,
}

/// Error base used in content filter results.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ErrorBase {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub code: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub message: Option<String>,
}

/// Severity result for a content filter category.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ContentFilterSeverityResult {
    pub filtered: bool,
    pub severity: ContentFilterSeverity,
}

/// Severity levels for content filtering.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum ContentFilterSeverity {
    Safe,
    Low,
    Medium,
    High,
}

/// Detected result for content filtering.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ContentFilterDetectedResult {
    pub filtered: bool,
    pub detected: bool,
}

/// Detected result with citation for content filtering.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ContentFilterDetectedWithCitationResult {
    pub filtered: bool,
    pub detected: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub citation: Option<ContentFilterCitation>,
}

/// Citation details for protected material detection.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ContentFilterCitation {
    #[serde(rename = "URL")]
    pub url: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub license: Option<String>,
}

/// Log probability information for a response choice.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseChoiceLogprob {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub content: Option<Vec<ResponseChoiceLogprobContent>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub refusal: Option<Vec<ResponseChoiceLogprobContent>>,
}

/// Log probability details for a token.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseChoiceLogprobContent {
    pub token: String,
    pub logprob: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub bytes: Option<Vec<u8>>,
    pub top_logprobs: Vec<ResponseChoiceTopLogprobContent>,
}

/// Top logprob candidates for a token.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseChoiceTopLogprobContent {
    pub token: String,
    pub logprob: f64,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub bytes: Option<Vec<u8>>,
}

/// Deprecated function call object on response messages.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseFunctionCall {
    pub name: String,
    pub arguments: String,
}

/// Tool call returned by the model.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum ResponseChoiceToolCall {
    #[serde(rename = "function", alias = "function")]
    Function { id: String, function: ResponseChoiceFunctionToolCall },
}

/// Function payload inside a tool call.
#[derive(Debug, PartialEq, Clone, Serialize, Deserialize)]
pub struct ResponseChoiceFunctionToolCall {
    pub name: String,
    pub arguments: String,
}
// endregion: --- Response structs

// region: --- Transform methods
pub mod to_openai_transform {
    use super::*;
    use crate::providers::openai::chat_completions::response::{
        Response as OpenAiResponse,
        ResponseUsage as OpenAiResponseUsage,
        ResponseChoice as OpenAiResponseChoice,
        ResponseChoiceMessage as OpenAiResponseChoiceMessage,
        ResponseChoiceToolCall as OpenAiResponseChoiceToolCall,
        ResponseChoiceFunctionToolCall as OpenAiResponseChoiceFunctionToolCall,
        CompletionTokensDetails as OpenAiCompletionTokensDetails,
    };
    use crate::providers::{Transformation, TransformationContext, TransformationLoss, Transformer};

    #[derive(Debug)]
    pub struct Loss {}

    #[derive(Debug)]
    pub struct Context {
        pub model: Option<String>
    }

    impl TransformationContext<Response, OpenAiResponse> for Context {}
    impl TransformationLoss<Response, OpenAiResponse> for Loss {}

    impl Transformer<OpenAiResponse, Context, Loss> for Response {
        fn transform(self, context: Context) -> Transformation<OpenAiResponse, Loss> {
            Transformation {
                result: OpenAiResponse {
                    id: self.id,
                    choices: self.choices.into_iter().map(|choice| transform_response_choice(choice)).collect(),
                    created: self.created,
                    model: context.model.unwrap_or(self.model),
                    system_fingerprint: self.system_fingerprint,
                    object: self.object,
                    usage: transform_response_usage(self.usage),
                    service_tier: None,
                },
                loss: Loss {},
            }
        }
    }

    fn transform_response_usage(usage: ResponseUsage) -> OpenAiResponseUsage {
        OpenAiResponseUsage {
            completion_tokens: usage.completion_tokens,
            prompt_tokens: usage.prompt_tokens,
            total_tokens: usage.total_tokens,
            completion_tokens_details: usage.completion_tokens_details.map(|details| {
                OpenAiCompletionTokensDetails {
                    accepted_prediction_tokens: None,
                    audio_tokens: None,
                    reasoning_tokens: details.reasoning_tokens,
                    rejected_prediction_tokens: None,
                }
            }),
            prompt_tokens_details: None,
        }
    }

    fn transform_response_choice(choice: ResponseChoice) -> OpenAiResponseChoice {
        OpenAiResponseChoice {
            finish_reason: choice.finish_reason,
            index: choice.index,
            message: transform_response_choice_message(choice.message),
            logprobs: None,
        }
    }

    fn transform_response_choice_message(message: ResponseChoiceMessage) -> OpenAiResponseChoiceMessage {
        OpenAiResponseChoiceMessage {
            content: message.content,
            role: message.role,
            tool_calls: message.tool_calls.map(|tool_calls| {
                tool_calls.into_iter().map(|tc| transform_response_choice_tool_call(tc)).collect()
            }),
            refusal: message.refusal,
            annotations: None,
            audio: None,
            function_call: message.function_call.map(|call| OpenAiResponseChoiceFunctionToolCall {
                name: call.name,
                arguments: call.arguments,
            }),
        }
    }

    fn transform_response_choice_tool_call(tool_call: ResponseChoiceToolCall) -> OpenAiResponseChoiceToolCall {
        match tool_call {
            ResponseChoiceToolCall::Function { id, function } => OpenAiResponseChoiceToolCall::Function {
                id,
                function: OpenAiResponseChoiceFunctionToolCall {
                    name: function.name,
                    arguments: function.arguments,
                },
            },
        }
    }
}
// endregion: --- Transform methods

#[cfg(test)]
mod tests {
    use super::Response;

    #[test]
    fn response_with_logprobs_and_context_parses() {
        let json = r#"{
            "id": "chatcmpl-1",
            "object": "chat.completion",
            "created": 10,
            "model": "gpt-4o",
            "choices": [{
                "finish_reason": "stop",
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "hello",
                    "refusal": null,
                    "tool_calls": [{
                        "type": "function",
                        "id": "call_1",
                        "function": { "name": "f", "arguments": "{}" }
                    }],
                    "function_call": { "name": "f", "arguments": "{}" },
                    "context": {
                        "intent": "greeting",
                        "citations": [{
                            "content": "doc",
                            "title": "t",
                            "url": "https://example.com"
                        }]
                    }
                },
                "logprobs": {
                    "content": [{
                        "token": "hello",
                        "logprob": -0.1,
                        "bytes": [104],
                        "top_logprobs": [{
                            "token": "hello",
                            "logprob": -0.1,
                            "bytes": [104]
                        }]
                    }],
                    "refusal": null
                }
            }],
            "usage": {
                "prompt_tokens": 1,
                "completion_tokens": 1,
                "total_tokens": 2
            }
        }"#;

        let response: Response = serde_json::from_str(json).expect("parse response");
        assert_eq!(response.choices.len(), 1);
        assert_eq!(
            response.choices[0]
                .message
                .context
                .as_ref()
                .and_then(|ctx| ctx.intent.as_deref()),
            Some("greeting")
        );
        assert!(response.choices[0].logprobs.is_some());
    }
}
